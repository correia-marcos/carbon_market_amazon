# =============================================================================
# Create dataset for the synthetic control
verra_875 <- final_data %>%
filter(ID == "875")
# Get the year of the beginning of the project
treatment_year <- 2009
# Add all donors
verra_875 <- rbind(verra_875, final_data %>%
filter(Treatment == 0 & REDD_type == "AUD"))
# Create the outcome column: protected_land
verra_875 <- mutate(verra_875, protected_land = natural_lands + forest)
# Subset data for the pre-treatment period
verra_875_pre <- verra_875[verra_875$year < treatment_year, ]
# Identify donors with no variation in the variable protected_land in the pre-treatment period
donors_no_variation <- unique(
verra_875_pre[verra_875_pre$protected_land == max(verra_875_pre$protected_land), "ID"])
# 3. Remove donors with no variation from the dataset
verra_875 <- verra_875[!verra_875$ID %in% donors_no_variation, ]
id.var      <- "ID"
time.var    <- "year"
period.pre  <- seq(from = 2008, to = treatment_year, by = 1)
period.post <- seq(from = treatment_year+1, to = 2021, by = 1)
unit.tr     <- "875"
unit.co     <- setdiff(unique(verra_875$ID), unit.tr)
outcome.var <- "protected_land"
cov.adj     <- NULL
features    <- NULL
constant    <- FALSE
report.missing <- FALSE
cointegrated.data <- TRUE
####################################
### Data preparation
df  <-   scdata(df = verra_875, id.var = id.var, time.var = time.var,
outcome.var = outcome.var, period.pre = period.pre,
period.post = period.post, unit.tr = unit.tr,
unit.co = unit.co, cov.adj = cov.adj, features = features,
constant = constant, cointegrated.data = cointegrated.data)
####################################
## Set options for inference
u.alpha  <- 0.05                         # Confidence level (in-sample uncertainty)
e.alpha  <- 0.05                         # Confidence level (out-of-sample uncertainty)
rho      <- NULL                         # Regularization parameter (if NULL it is estimated)
rho.max  <- 1                            # Maximum value attainable by rho
sims     <- 200                          # Number of simulations
V        <- NULL                         # Weighting matrix (if NULL it is the identity matrix)
u.order  <- 1                            # Degree of polynomial in B and C when modelling u
u.lags   <- 0                            # Lags of B to be used when modelling u
u.sigma  <- "HC1"                        # Estimator for the variance-covariance of u
u.missp  <- T                            # If TRUE then the model is treated as misspecified
e.lags   <- 0                            # Degree of polynomial in B and C when modelling e
e.order  <- 1                            # Lags of B to be used when modelling e
e.method <- "gaussian"                   # Estimation method for out-of-sample uncertainty
cores    <- 3                            # Number of cores to be used by scpi
w.constr <- list(name = "simplex")       # Simplex-type constraint set
# Results
set.seed(8894)
result  <- scpi(data = df,u.order = u.order, u.lags = u.lags, u.sigma = u.sigma,
u.missp = u.missp, sims = sims, e.order = e.order,
e.lags = e.lags, e.method = e.method, cores = cores,
w.constr = w.constr, u.alpha = u.alpha, e.alpha = e.alpha,
rho = rho, rho.max = rho.max)
verra_1113 <- final_data %>%
filter(ID == "1113")
# Get the year of the beginning of the project
treatment_year <- 2011
# Add all donors
verra_1113 <- rbind(verra_1113, final_data %>%
filter(Treatment == 0 & REDD_type == "AUD"))
# Create the outcome column: protected_land
verra_1113 <- mutate(verra_1113, protected_land = natural_lands + forest)
# Subset data for the pre-treatment period
verra_1113_pre <- verra_1113[verra_1113$year < treatment_year, ]
# Identify donors with no variation in the variable protected_land in the pre-treatment period
donors_no_variation <- unique(
verra_1113_pre[verra_1113_pre$protected_land == max(verra_1113_pre$protected_land), "ID"])
# 3. Remove donors with no variation from the dataset
verra_1113 <- verra_1113[!verra_1113$ID %in% donors_no_variation, ]
####################################
### Set options for data preparation
id.var      <- "ID"
time.var    <- "year"
period.pre  <- seq(from = 2008, to = treatment_year, by = 1)
period.post <- seq(from = treatment_year+1, to = 2021, by = 1)
unit.tr     <- "1113"
unit.co     <- setdiff(unique(verra_1113$ID), unit.tr)
outcome.var <- "protected_land"
cov.adj     <- NULL
features    <- NULL
constant    <- FALSE
report.missing <- FALSE
cointegrated.data <- TRUE
####################################
### Data preparation
df  <-   scdata(df = verra_1113, id.var = id.var, time.var = time.var,
outcome.var = outcome.var, period.pre = period.pre,
period.post = period.post, unit.tr = unit.tr,
unit.co = unit.co, cov.adj = cov.adj, features = features,
constant = constant, cointegrated.data = cointegrated.data)
####################################
## Set options for inference
u.alpha  <- 0.05                         # Confidence level (in-sample uncertainty)
e.alpha  <- 0.05                         # Confidence level (out-of-sample uncertainty)
rho      <- NULL                         # Regularization parameter (if NULL it is estimated)
rho.max  <- 1                            # Maximum value attainable by rho
sims     <- 200                          # Number of simulations
V        <- NULL                         # Weighting matrix (if NULL it is the identity matrix)
u.order  <- 1                            # Degree of polynomial in B and C when modelling u
u.lags   <- 0                            # Lags of B to be used when modelling u
u.sigma  <- "HC1"                        # Estimator for the variance-covariance of u
u.missp  <- T                            # If TRUE then the model is treated as misspecified
e.lags   <- 0                            # Degree of polynomial in B and C when modelling e
e.order  <- 1                            # Lags of B to be used when modelling e
e.method <- "gaussian"                   # Estimation method for out-of-sample uncertainty
cores    <- 3                            # Number of cores to be used by scpi
w.constr <- list(name = "simplex")       # Simplex-type constraint set
# Results
set.seed(8894)
result  <- scpi(data = df,u.order = u.order, u.lags = u.lags, u.sigma = u.sigma,
u.missp = u.missp, sims = sims, e.order = e.order,
e.lags = e.lags, e.method = e.method, cores = cores,
w.constr = w.constr, u.alpha = u.alpha, e.alpha = e.alpha,
rho = rho, rho.max = rho.max)
# =============================================================================
# Synthetic control estimation
# =============================================================================
# =============================================================================
# Create dataset for the synthetic control
verra_1115 <- final_data %>%
filter(ID == "1115")
# Get the year of the beginning of the project
treatment_year <- 2009
# Get the year of the beginning of the project
treatment_year <- 2011
# Add all donors
verra_1115 <- rbind(verra_1115, final_data %>%
filter(Treatment == 0 & REDD_type == "AUD"))
# Create the outcome column: protected_land
verra_1115 <- mutate(verra_1115, protected_land = natural_lands + forest)
# Subset data for the pre-treatment period
verra_1115_pre <- verra_1115[verra_1115$year < treatment_year, ]
# Identify donors with no variation in the variable protected_land in the pre-treatment period
donors_no_variation <- unique(
verra_1115_pre[verra_1115_pre$protected_land == max(verra_1115_pre$protected_land), "ID"])
# 3. Remove donors with no variation from the dataset
verra_1115 <- verra_1115[!verra_1115$ID %in% donors_no_variation, ]
id.var      <- "ID"
time.var    <- "year"
period.pre  <- seq(from = 2008, to = treatment_year, by = 1)
period.post <- seq(from = treatment_year+1, to = 2021, by = 1)
unit.tr     <- "1115"
unit.co     <- setdiff(unique(verra_1115$ID), unit.tr)
outcome.var <- "protected_land"
cov.adj     <- NULL
features    <- NULL
constant    <- FALSE
report.missing <- FALSE
cointegrated.data <- TRUE
####################################
### Data preparation
df  <-   scdata(df = verra_1115, id.var = id.var, time.var = time.var,
outcome.var = outcome.var, period.pre = period.pre,
period.post = period.post, unit.tr = unit.tr,
unit.co = unit.co, cov.adj = cov.adj, features = features,
constant = constant, cointegrated.data = cointegrated.data)
####################################
## Set options for inference
u.alpha  <- 0.05                         # Confidence level (in-sample uncertainty)
e.alpha  <- 0.05                         # Confidence level (out-of-sample uncertainty)
rho      <- NULL                         # Regularization parameter (if NULL it is estimated)
rho.max  <- 1                            # Maximum value attainable by rho
sims     <- 200                          # Number of simulations
V        <- NULL                         # Weighting matrix (if NULL it is the identity matrix)
u.order  <- 1                            # Degree of polynomial in B and C when modelling u
u.lags   <- 0                            # Lags of B to be used when modelling u
u.sigma  <- "HC1"                        # Estimator for the variance-covariance of u
u.missp  <- T                            # If TRUE then the model is treated as misspecified
e.lags   <- 0                            # Degree of polynomial in B and C when modelling e
e.order  <- 1                            # Lags of B to be used when modelling e
e.method <- "gaussian"                   # Estimation method for out-of-sample uncertainty
cores    <- 3                            # Number of cores to be used by scpi
w.constr <- list(name = "simplex")       # Simplex-type constraint set
# Results
set.seed(8894)
result  <- scpi(data = df,u.order = u.order, u.lags = u.lags, u.sigma = u.sigma,
u.missp = u.missp, sims = sims, e.order = e.order,
e.lags = e.lags, e.method = e.method, cores = cores,
w.constr = w.constr, u.alpha = u.alpha, e.alpha = e.alpha,
rho = rho, rho.max = rho.max)
y.fit <- rbind(result$est.results$Y.pre.fit, result$est.results$Y.post.fit)
y.act <- rbind(result$data$Y.pre, result$data$Y.post)
sc.l  <- result$inference.results$CI.all.gaussian[, 1, drop = FALSE]
sc.r  <- result$inference.results$CI.all.gaussian[, 2, drop = FALSE]
# Store other specifics
period.pre  <- result$data$specs$period.pre
period.post <- result$data$specs$period.post
T0          <- period.pre[length(period.pre)] # intercept
plot.range  <- c(period.pre, period.post)
# Actual data
dat    <- data.frame(t     = c(period.pre, period.post),
Y.act = c(y.act),
sname = "Treated")
# Fill with NAs Y.fit and confidence bounds where missing
Y.fit.na  <- matrix(NA, nrow = length(c(period.pre, period.post)))
sc.l.na   <- matrix(NA, nrow = length(c(period.pre, period.post)))
sc.r.na   <- matrix(NA, nrow = length(c(period.pre, period.post)))
names <- strsplit(rownames(y.fit), "\\.")
not.missing.plot <- c(period.pre,period.post) %in% unlist(lapply(names, "[[", 2))
names <- strsplit(rownames(sc.l), "\\.")
not.missing.ci   <- c(period.pre,period.post) %in% unlist(lapply(names, "[[", 2))
Y.fit.na[not.missing.plot, 1] <- y.fit
sc.l.na[not.missing.ci, 1]    <- sc.l
sc.r.na[not.missing.ci, 1]    <- sc.r
# Synthetic unit data
dat.sc <- data.frame(t        = c(period.pre, period.post),
Y.sc     = Y.fit.na,
lb       = c(sc.l.na), ub = c(sc.r.na),
sname    = "SC Unit")
# Set ticks, event label and merge
x.ticks <- c(seq(plot.range[1], plot.range[length(plot.range)], length.out = 5), T0)
x.ticks <- round(unique(x.ticks))
event.lab <- paste("\n", "REDD+", sep = "")
event.lab.height <- 1
dat.plot    <- subset(dat,    t %in% plot.range)
dat.sc.plot <- subset(dat.sc, t %in% plot.range)
plotdf <- dplyr::left_join(dat.plot, dat.sc.plot, by = 't')
## Plot specs
plot <- ggplot() + theme_bw() +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1)) +
labs(x = "Year", y = "% of Forest and Natural Formation") +
theme(legend.position = "bottom", legend.box = "horizontal", legend.title = element_blank(),
legend.background = element_rect(fill = "white", color = "black"))
## Add Series to plot
plot <- plot +
geom_line( data = plotdf, aes(x = t, y = Y.act, colour = sname.x), linetype = 'solid') +
geom_point(data = plotdf, aes(x = t, y = Y.act, colour = sname.x), shape = 1) +
geom_line( data = plotdf, aes(x = t, y = Y.sc,  colour = sname.y), linetype = 'dashed') +
geom_point(data = plotdf, aes(x = t, y = Y.sc,  colour = sname.y), shape = 19) +
geom_vline(xintercept = T0, linetype = "dashed") +
geom_text(aes(x = T0, label = event.lab, y = event.lab.height), angle = 90, size = 4) +
scale_x_continuous(breaks = x.ticks) +
scale_color_manual(name = "", values = c("mediumblue", "grey46"),
labels = c("Synthetic Control", "Treated"),
guide = guide_legend(override.aes = list(
linetype = c('dashed','solid'), shape = c(19, 1))))
## Add confidence bars and plot
plot + geom_errorbar(data = plotdf,
aes(x = t, ymin = lb, ymax = ub, colour = sname.y),
width = 0.5, linetype = 1)
mean_registrados <- mean(registered$forest)
mean(registered$forest)
colnames(registered)
mean(registered$forest)
mean(registered$natural_lands)
mean(registered$agriculture)
mean(registered$pasture)
mean(not_registered$forest)
mean(not_registered$natural_lands)
mean(not_registered$agriculture)
mean(not_registered$pasture)
sd(not_registered$pasture)
sd(not_registered$agriculture)
sd(not_registered$natural_lands)
sd(not_registered$forest)
sd(registered$forest)
sd(registered$natural_lands)
sd(registered$agriculture)
sd(registered$pasture)
max(registered$forest)
max(registered$natural_lands)
max(registered$agriculture)
max(registered$pasture)
max(not_registered$forest)
max(not_registered$natural_lands)
max(not_registered$agriculture)
max(not_registered$pasture)
min(not_registered$pasture)
min(not_registered$agriculture)
min(not_registered$natural_lands)
min(not_registered$forest)
min(registered$forest)
min(registered$natural_lands)
min(registered$agriculture)
projects <- sf::st_read("Data/Final_base")
# Required for increasing reproducibility
library(groundhog)            # You need to have at least R version = 4.3.1
# Required packages
pkgs <- c("dplyr", "readxl", "lubridate", "data.table")
# Load packages
groundhog.library(pkgs, "2023-09-01")
# Import data: excel previous created by author
data <- readxl::read_excel("Data/Projects_info/base_projects.xlsx",
na = "--")
# Substituting white spaces per underline
colnames(data) <- gsub(" ", "_", colnames(data))
# List of of files with issues information
issues_files <- list.files("Data/Projects_info/Issuances", full.names = TRUE)
# Read data from Excel file
project_data <- read_excel(issues_files[116])
# Substituting white spaces per underline
colnames(project_data) <- gsub(" ", "_", colnames(project_data))
# Filter dataset
filtered_data <- project_data %>%
select(c("Issuance_Date", "Vintage_Start", "ID",
"Retirement/Cancellation_Date", "Quantity_Issued"))
# Create separate data frames for each date type
issuance_df <- filtered_data %>%
mutate(year = year(Issuance_Date)) %>%
group_by(year) %>%
summarise(ID = first(ID), issued_credits = sum(Quantity_Issued))
vintage_df <- filtered_data %>%
mutate(year = year(Vintage_Start)) %>%
group_by(year) %>%
summarise(ID = first(ID), vintage_credits = sum(Quantity_Issued))
retirement_df <- filtered_data %>%
mutate(year = year(`Retirement/Cancellation_Date`)) %>%
filter(!is.na(year)) %>%  # Removing NA values - happens in the year column
group_by(year) %>%
summarise(ID = first(ID), retired_credits = sum(Quantity_Issued))
Reduce(function(x, y) merge(x, y, all=TRUE), df_list)
# Set all data frames into list and then merge all data frames in list
list_df <- list(issuance_df, vintage_df, retirement_df)
Reduce(function(x, y) merge(x, y, all=TRUE), list_df)
final_df <- Reduce(function(x, y) merge(x, y, all=TRUE), list_df)
View(final_df)
# Return final_df
return(final_df)
process_project_info <- function(file_path) {
# Read data from Excel file
project_data <- read_excel(file_path)
# Substituting white spaces per underline
colnames(project_data) <- gsub(" ", "_", colnames(project_data))
# Filter dataset
filtered_data <- project_data %>%
select(c("Issuance_Date", "Vintage_Start", "ID",
"Retirement/Cancellation_Date", "Quantity_Issued"))
# Create separate data frames for each date type
issuance_df <- filtered_data %>%
mutate(year = year(Issuance_Date)) %>%
group_by(year) %>%
summarise(ID = first(ID), issued_credits = sum(Quantity_Issued))
vintage_df <- filtered_data %>%
mutate(year = year(Vintage_Start)) %>%
group_by(year) %>%
summarise(ID = first(ID), vintage_credits = sum(Quantity_Issued))
retirement_df <- filtered_data %>%
mutate(year = year(`Retirement/Cancellation_Date`)) %>%
filter(!is.na(year)) %>%  # Removing NA values - happens in the year column
group_by(year) %>%
summarise(ID = first(ID), retired_credits = sum(Quantity_Issued))
# Set all data frames into list and then merge all data frames in list
list_df <- list(issuance_df, vintage_df, retirement_df)
final_df <- Reduce(function(x, y) merge(x, y, all=TRUE), list_df)
# Return final_df
return(final_df)
}
teste <- process_project_info(issues_files[116])
View(teste)
View(teste)
# Loop through files and collect data
all_project_data <- lapply(issues_files, process_project_info)
# Combine data from all files
data_final <- rbindlist(all_project_data)
View(data_final)
lifecycle::last_lifecycle_warnings()
# Read data from Excel file
project_data <- read_excel(issues_files[97])
View(project_data)
# Substituting white spaces per underline
colnames(project_data) <- gsub(" ", "_", colnames(project_data))
View(project_data)
# Filter dataset
filtered_data <- project_data %>%
select(c("Issuance_Date", "Vintage_Start", "ID",
"Retirement/Cancellation_Date", "Quantity_Issued"))
# Create separate data frames for each date type
issuance_df <- filtered_data %>%
mutate(year = year(Issuance_Date)) %>%
group_by(year) %>%
summarise(ID = first(ID), issued_credits = sum(Quantity_Issued))
vintage_df <- filtered_data %>%
mutate(year = year(Vintage_Start)) %>%
group_by(year) %>%
summarise(ID = first(ID), vintage_credits = sum(Quantity_Issued))
retirement_df <- filtered_data %>%
mutate(year = year(`Retirement/Cancellation_Date`)) %>%
filter(!is.na(year)) %>%  # Removing NA values - happens in the year column
group_by(year) %>%
summarise(ID = first(ID), retired_credits = sum(Quantity_Issued))
View(retirement_df)
# Set all data frames into list and then merge all data frames in list
list_df <- list(issuance_df, vintage_df, retirement_df)
final_df <- Reduce(function(x, y) merge(x, y, all=TRUE), list_df)
# Get the VERRA ID
id <- unique(filtered_data$ID)
# Substituting white spaces per underline
colnames(project_data) <- gsub(" ", "_", colnames(project_data))
# Filter dataset
filtered_data <- project_data %>%
select(c("Issuance_Date", "Vintage_Start", "ID",
"Retirement/Cancellation_Date", "Quantity_Issued"))
# Get the VERRA ID
id <- unique(filtered_data$ID)
# Create separate data frames for each date type
issuance_df <- filtered_data %>%
mutate(year = year(Issuance_Date)) %>%
group_by(year) %>%
summarise(issued_credits = sum(Quantity_Issued)) %>%
mutate(ID = id) # Get the Verra ID
View(issuance_df)
vintage_df <- filtered_data %>%
mutate(year = year(Vintage_Start)) %>%
group_by(year) %>%
summarise(vintage_credits = sum(Quantity_Issued)) %>%
mutate(ID = id) # Get the Verra ID
View(vintage_df)
retirement_df <- filtered_data %>%
mutate(year = year(`Retirement/Cancellation_Date`)) %>%
filter(!is.na(year)) %>%  # Removing NA values - happens in the year column
group_by(year) %>%
summarise(retired_credits = sum(Quantity_Issued)) %>%
mutate(ID = id)
View(retirement_df)
# Set all data frames into list and then merge all data frames in list
list_df <- list(issuance_df, vintage_df, retirement_df)
final_df <- Reduce(function(x, y) merge(x, y, all=TRUE), list_df)
View(final_df)
process_project_info <- function(file_path) {
# Read data from Excel file
project_data <- read_excel(file_path)
# Substituting white spaces per underline
colnames(project_data) <- gsub(" ", "_", colnames(project_data))
# Filter dataset
filtered_data <- project_data %>%
select(c("Issuance_Date", "Vintage_Start", "ID",
"Retirement/Cancellation_Date", "Quantity_Issued"))
# Get the VERRA ID
id <- unique(filtered_data$ID)
# Create separate data frames for each date type
issuance_df <- filtered_data %>%
mutate(year = year(Issuance_Date)) %>%
group_by(year) %>%
summarise(issued_credits = sum(Quantity_Issued)) %>%
mutate(ID = id) # Get the Verra ID
vintage_df <- filtered_data %>%
mutate(year = year(Vintage_Start)) %>%
group_by(year) %>%
summarise(vintage_credits = sum(Quantity_Issued)) %>%
mutate(ID = id) # Get the Verra ID
retirement_df <- filtered_data %>%
mutate(year = year(`Retirement/Cancellation_Date`)) %>%
filter(!is.na(year)) %>%  # Removing NA values - happens in the year column
group_by(year) %>%
summarise(retired_credits = sum(Quantity_Issued)) %>%
mutate(ID = id)
# Set all data frames into list and then merge all data frames in list
list_df <- list(issuance_df, vintage_df, retirement_df)
final_df <- Reduce(function(x, y) merge(x, y, all=TRUE), list_df)
# Return the final dataframe
return(final_df)
}
# Loop through files and collect data
all_project_data <- lapply(issues_files, process_project_info)
# Combine data from all files
data_final <- rbindlist(all_project_data)
View(data_final)
class(data_final$issued_credits)
class(data_final$vintage_credits)
class(data_final$retired_credits)
rm(data_final)
# Combine data from all files
projects_issues <- rbindlist(all_project_data)
# Save dataframe
write.csv(projects_issues, file = "Data/Projects_info/projects_issues.csv",
row.names = FALSE)
# Required for increasing reproducibility
library(groundhog)            # You need to have at least R version = 4.3.1
# Required packages
pkgs <- c("ggplot2", "dplyr", "sf", "sp")
# Load packages
groundhog.library(pkgs, "2023-09-01")
# Use this line to avoid geometries issues - allow for self-intersection
sf_use_s2(FALSE)
# Get a list of the states to get CAR information
states <- list.files("Data/CAR")
# Import projects and the csv data
projects <- sf::st_read("Results/base_afolu_complete")
View(projects)
# Get only the REDD+ projects and change its CRS
projects_redd <- projects[projects$afolu == "REDD", ]
# Make shapefiles valid
projects_redd <- sf::st_make_valid(projects_redd)
sf::st_is_valid(projects_redd)
# Read the CAR file for each state
car <- sf::st_read(paste0("Data/CAR/", states[1]))
View(car)
# Make shapefiles valid and convert CRS to the same as in REDD+ projects
car <- sf::st_make_valid(car)
car <- st_transform(car, crs = st_crs(projects_redd))
# Check if a project is within a property on state CAR
intersections <- st_join(projects_redd, car)
test <- intersections[intersections$id_rgst == "4089", ]
View(test)
head(intersections)
head(intersections, n = 20)
head(intersections, n = 20)
head(test)
test$name
test$gid
hehe <- sf::st_read("Data/ATLAS/AC/")
View(hehe)
unique(hehe$nm_subclass)
